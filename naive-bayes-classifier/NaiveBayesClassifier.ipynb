{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<h1> Naives Bayer Classifier </h1>\n",
    "</center>\n",
    "\n",
    "### Classification\n",
    "The idea of any classification is to discriminate datapoints of a set, by assigning them a class. This discrimination is based on the given characteristics or attributes. <br>\n",
    "In other words, given a training set and a set of hypothesis, we want to find the most probable hypothesis. <br>\n",
    "In mathematical terms, given an example described by a set of $p$  attributes $\\{a_1,...a_p\\}$,we want to find the optimal class: \n",
    "\n",
    "\\begin{align}\n",
    "    c_{opt} = argmax_{c_j \\in C}(P(c_j|a_1,...,a_p))\n",
    "    \\label{eq1}\n",
    "    \\tag{1}\n",
    "\\end{align}\n",
    "Where: <br>\n",
    "$C$ is the set of classes composed by $c_1,...,c_j$\n",
    "\n",
    "### Naive Bayes\n",
    "The Naive Bayes Theorem proposes a formula to calculate the probabilities of the different hypothesis.\n",
    "\n",
    "\\begin{align}\n",
    "    c_{opt} = argmax_{c_j \\in C}\\frac{P(a_1,...,a_p|c_j)P(c_j)}{P(a_1,...,a_p)}\n",
    "    \\label{eq2}\n",
    "    \\tag{2}\n",
    "\\end{align}\n",
    "\n",
    "As we only care about finding the maximum argument, we can get rid of the denumerator.\n",
    "\n",
    "\\begin{align}\n",
    "    c_{opt} = argmax_{c_j \\in C}P(a_1,...,a_p|c_j)P(c_j)\n",
    "    \\label{eq3}\n",
    "    \\tag{3}\n",
    "\\end{align}\n",
    "  \n",
    "This *naive* Bayes classifier assumes that the values of the attributes are independent, given the class. In mathematic terms: \n",
    "\\begin{align}\n",
    "    P(a_1,...,a_p|c_j)=\\prod_iP(a_i|c_j)\n",
    "    \\label{eq4}\n",
    "    \\tag{4}\n",
    "\\end{align}\n",
    "  \n",
    "\n",
    "Thus, replacing \\ref{eq4} in \\ref{eq3}, we obtain:\n",
    "\\begin{align}\n",
    "    c_{opt} = argmax_{c_j \\in C}\\prod_iP(a_i|c_j)P(c_j)\n",
    "    \\label{eq5}\n",
    "    \\tag{5}\n",
    "\\end{align}\n",
    "\n",
    "Where:<br>\n",
    "$P(c_j)$ are the classes prior probability (or \"a priori\" probabilities) <br>\n",
    "$P(a_i|c_j)$ are the attributes conditional probabilities, given the class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective\n",
    "The objective is to implement a naive bayes classifier froms scratch, that can predict/classify if a person is either British or Scotish, taking into account his/her preferences. <br>\n",
    "The independent variables are the preferences:<br>\n",
    "$scones \\in \\{0,1\\}$<br>\n",
    "$beer \\in \\{0,1\\}$<br>\n",
    "$whisky \\in \\{0,1\\}$<br>\n",
    "$oatmeal \\in \\{0,1\\}$<br>\n",
    "$football \\in \\{0,1\\}$<br>\n",
    "Where 0 indicates that the person dislikes that attribute and 1 indicates that the person does like it. <br>\n",
    "\n",
    "The hypothesis space C includes the two different classes (the nationalities) that can be predicted: English ($E$) or Scotish ($S$): <br>\n",
    "$C=\\{E,S\\}$\n",
    "\n",
    "Throughout this notebook, the terms 'independent variables', 'attributes', or 'preferences' are used to refer to the same concept. With regards the hipothesis space, the terms 'dependent variable', 'output', 'class' and 'nationality' are used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "A simple database was created for training the classifier.<br>\n",
    "Each column represents one attribute (independent variables).<br>\n",
    "The last comun represents the class/nationality (dependent variable).<br>\n",
    "Each row corresponds to each of the training datapoints. <br>\n",
    "\n",
    "As the aim of this notebook is to understand how a Naive Bayesian classifier works, only a small and invented database is used. In order to test the performance of out classifier, a larger dataset would be needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    scones  beer  whisky  oatmeal  football nationality\n",
      "0        0     0       1        1         1           E\n",
      "1        1     0       1        1         0           E\n",
      "2        1     1       0        0         1           E\n",
      "3        1     1       0        0         0           E\n",
      "4        0     1       0        0         1           E\n",
      "5        0     0       0        1         0           E\n",
      "6        1     0       0        1         1           S\n",
      "7        1     1       0        0         1           S\n",
      "8        1     1       1        1         0           S\n",
      "9        1     1       0        1         0           S\n",
      "10       1     1       0        1         1           S\n",
      "11       1     0       1        1         0           S\n",
      "12       1     0       1        0         0           S\n"
     ]
    }
   ],
   "source": [
    "# Read an Excel file into a pandas DataFrame.\n",
    "data = pd.read_excel(\"preferences.xlsx\") \n",
    "# Dependent variable name \n",
    "hypothesis = list(data.columns)[-1]\n",
    "# List of independent variable names\n",
    "attributes = list(data.columns)[:-1] \n",
    "print (data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We transform our data into numpy matrices, to follow the formats that most machine learning libraries used.<br>\n",
    "$n$: number of datapoints in our training set.<br>\n",
    "$p$: number of attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some variables to better explore the dataset\n",
    "X_train = np.array(data[attributes])\n",
    "y_train = np.array(data[hypothesis])\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "p = len(data.columns)-1\n",
    "n = len(data.index)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Priori Class Probabilities\n",
    "In this case the \"a priori\" probability of being from a specific class (nationality) is estimated by counting the number of that specific class and dividing it by the total number of samples in our training set. <br>\n",
    "However, in another problem, this probabilties could be known information (with no need to estimate it from the dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_probs(data):\n",
    "    ''' Given a training dataset, return the count and the 'a priori' probabilities for each class. '''\n",
    "    # Array to keep track of the counts\n",
    "    class_count = np.zeros((1,len(classes)))\n",
    "    class_prior = np.zeros((1,len(classes)))\n",
    "    for j in range(len(classes)):\n",
    "        class_count[0][j] = len([i for i in data[hypothesis] if i == classes[j]])\n",
    "        class_prior[0][j] = class_count[0][j]/n\n",
    "    return class_count, class_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 'a priori' probabilites are P(E) = 0.462 and P(S) = 0.538\n"
     ]
    }
   ],
   "source": [
    "class_count, class_prior = prior_probs(data)\n",
    "print(f\"The 'a priori' probabilites are P(E) = {class_prior[0][0]:.3f} and P(S) = {class_prior[0][1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conditional A Priori Attribute Probabilities\n",
    "##### For the english class\n",
    "- `eprob0`: The probabilites of not liking each attribute, given that the person is english.\n",
    "- `eprob1`: The probabilty of liking each attribute, given that the person is english.\n",
    "\n",
    "Laplacian Correction: we add 1 in the numerator and p=2 in the denominator, to deal with 0 probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5, 0.5, 0.625, 0.5, 0.5]\n",
      "[0.5, 0.5, 0.375, 0.5, 0.5]\n"
     ]
    }
   ],
   "source": [
    "eprob0 = []   \n",
    "for col in range(0, p):\n",
    "    ecount0 = 0\n",
    "    for row in range(0,n):\n",
    "        if X_train[row,col] == 0 and y_train[row] == classes[0]:\n",
    "            ecount0 += 1\n",
    "    # Append to list with Laplace correction\n",
    "    eprob0.append((ecount0+1)/(class_count[0][0]+2))\n",
    "    \n",
    "eprob1 = [(1-i) for i in eprob0] \n",
    "print(eprob0)\n",
    "print(eprob1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For the Scottish class\n",
    "- `sprob0`: The probabilites of not liking each attribute, given that the person is Scottish.\n",
    "- `sprob1`: The probabilty of liking each attribute, given that the person is scottish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.1111111111111111, 0.4444444444444444, 0.5555555555555556, 0.3333333333333333, 0.5555555555555556]\n",
      "[0.8888888888888888, 0.5555555555555556, 0.4444444444444444, 0.6666666666666667, 0.4444444444444444]\n"
     ]
    }
   ],
   "source": [
    "sprob0 = []   \n",
    "for col in range(0, p):\n",
    "    scount0 = 0\n",
    "    for row in range(0,n):\n",
    "        if X_train[row,col] == 0 and y_train[row] == classes[1]:\n",
    "            scount0 += 1\n",
    "    # Append to list with Laplace correction\n",
    "    sprob0.append((scount0+1)/(class_count[0][1]+2))\n",
    "    \n",
    "sprob1 = [(1-i) for i in sprob0] \n",
    "print(sprob0)\n",
    "print(sprob1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These probabilities can be visualized in the following table for better understanding.\n",
    "\n",
    "<img src=\"probs.png\" width=400/>\n",
    "\n",
    "For example:<br>\n",
    "$P(o=1|S)$ is read: The probability that the person likes oatmeal, given that he/she is Scottish.<br>\n",
    "$P(f=0|E)$ is read: The probability that the person doesn't like football, given that he/she is English.\n",
    "## Apply Bayes Formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes(input_vector):\n",
    "    prob_vector_e = 1\n",
    "    prob_vector_s = 1\n",
    "    for i in range(0,len(input_vector)):\n",
    "        if input_vector[i] == 0:\n",
    "            eprob = eprob0[i]\n",
    "            sprob = sprob0[i]\n",
    "        elif input_vector[i] == 1:\n",
    "            eprob = eprob1[i]\n",
    "            sprob = sprob1[i]\n",
    "        prob_vector_e = prob_vector_e*eprob\n",
    "        prob_vector_s = prob_vector_s*sprob\n",
    "    prob_vector_e = prob_vector_e*class_prior[0][0]\n",
    "    prob_vector_s = prob_vector_s*class_prior[0][1]\n",
    "    \n",
    "    #  Optimal Class  \n",
    "    if prob_vector_e > prob_vector_s:\n",
    "        return print(\"Given the attributes, the person is English\")\n",
    "    elif prob_vector_s > prob_vector_e:\n",
    "         return print(\"Given the attributes, the person is Scottish\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given the attributes, the person is Scottish\n"
     ]
    }
   ],
   "source": [
    "inputVector = [1, 1, 1, 1, 1]\n",
    "bayes(inputVector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayes' Main Dissadvantages\n",
    "\n",
    "- It works if the attributes are independent between them.\n",
    "- The computational cost of calculating all the probabilitie might be high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
